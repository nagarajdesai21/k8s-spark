# gcs configs
# spark.kubernetes.file.upload.path = gs://your-gcs-bucket/spark-upload/
# spark.jars.packages = com.google.cloud.bigdataoss:gcs-connector:hadoop3-2.2.19
# spark.hadoop.fs.gs.impl = com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
# spark.hadoop.google.cloud.auth.service.account.enable = false

spark.hadoop.fs.s3a.access.key=qJ1e6yPD63g5iM9ZG7Pl
spark.hadoop.fs.s3a.secret.key=mBDBPHVA7DlnVW9fnJgSzoXnGgFPFzQkb4WYd6iB
spark.hadoop.fs.s3a.endpoint=http://localhost:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false


spark.master = k8s://http://127.0.0.1:8001
spark.submit.deployMode = cluster
spark.kubernetes.container.image = apache/spark:3.5.1
spark.kubernetes.namespace = default
spark.kubernetes.authenticate.driver.serviceAccountName = spark
# MinIO configs
spark.hadoop.fs.s3a.access.key=qJ1e6yPD63g5iM9ZG7Pl
spark.hadoop.fs.s3a.secret.key=mBDBPHVA7DlnVW9fnJgSzoXnGgFPFzQkb4WYd6iB
spark.hadoop.fs.s3a.endpoint=http://localhost:9000
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false

# Required for uploading local files to cluster
# spark.kubernetes.file.upload.path=s3a://<your-bucket-name>/spark-upload
spark.kubernetes.file.upload.path=file:///opt/

# spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
# spark.hadoop.fs.AbstractFileSystem.s3a.impl=org.apache.hadoop.fs.s3a.S3A
# spark.hadoop.fs.s3a.fast.upload=true

spark.kubernetes.caCertFile=/root/.minikube/ca.crt
spark.kubernetes.client.keyFile=/root/.minikube/profiles/minikube/client.key
spark.kubernetes.client.certFile=/root/.minikube/profiles/minikube/client.crt
spark.kubernetes.authenticate.driver.skipTlsVerify=true
spark.kubernetes.authenticate.submission.skipTlsVerify=true
spark.kubernetes.authenticate.submission.ssl.hostnameVerification=false